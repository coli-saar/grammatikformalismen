
## change to True to print debugging output
## (feel free to access this from your own code)
debug: False

## the batch size to use during training
batchsize: 16

## number of times the training should iterate
## over the training data
epochs: 1000

## activation function
activation: relu

## learning rate
learning_rate: 0.0002

## beta hyperparameters for the Adam optimizer
betas: [0.9, 0.999]

## name of the Weights & Biases project into which your
## experiment will be logged
wandb_project: "dependency-parser"

## max number of training instances that will be used in each epoch;
## feel free to reduce this (e.g. to 500) during development
limit_train: 1000000000

## max number of development instances that will be used to evaluate
## the model after each epoch; feel free to reduce this (e.g. to 100)
## during development
limit_dev: 1000000000
